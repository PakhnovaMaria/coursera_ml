{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Импортируем необходимые библиотеки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Импортируем warnings filter для игнорирования ошибок\n",
    "from warnings import simplefilter\n",
    "simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Загружаем данные"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                   0  1\n",
      "0          2 . take around 10,000 640x480 pictures .  1\n",
      "1  i downloaded a trial version of computer assoc...  1\n",
      "2  the wrt54g plus the hga7t is a perfect solutio...  1\n",
      "3  i dont especially like how music files are uns...  0\n",
      "4  i was using the cheapie pail ... and it worked...  1\n",
      "(2000, 2)\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv('products_sentiment_train.tsv', sep = '\\t', header = None)\n",
    "print(train.head(5))\n",
    "print(train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Создаем список из текстов всех имеющихся отзывов (texts), а также список с классами, которые будет использовать классификатор (labels) - 0 для негативных отзывов и 1 для позитивных."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2 . take around 10,000 640x480 pictures .', 'i downloaded a trial version of computer associates ez firewall and antivirus and fell in love with a computer security system all over again .', 'the wrt54g plus the hga7t is a perfect solution if you need wireless coverage in a wider area or for a hard-walled house as was my case .', 'i dont especially like how music files are unstructured ; basically they are just dumped into one folder with no organization , like you might have in windows explorer folders and subfolders .', 'i was using the cheapie pail ... and it worked ok until the opening device fell apart .']\n",
      "\n",
      "[1, 1, 1, 0, 1]\n"
     ]
    }
   ],
   "source": [
    "texts = list(train[0][:])\n",
    "labels = list(train[1][:])\n",
    "print(texts[:5])\n",
    "print()\n",
    "print(labels[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Количество отзывов = 2000\n",
      "Доля класса 1 в выборке = 0.637\n"
     ]
    }
   ],
   "source": [
    "print('Количество отзывов =', len(texts))\n",
    "print('Доля класса 1 в выборке =', sum(labels)/len(labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Здесь и далее оценка качества будет выполняться с помощью cross_val_score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Оценка качества работы разных классификаторов\n",
    "\n",
    "def text_classifier(vectorizer, classifier):\n",
    "    return Pipeline([(\"vectorizer\", vectorizer),\n",
    "                    (\"classifier\", classifier)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vectorizer - <class 'sklearn.feature_extraction.text.CountVectorizer'>\n",
      "classifier - <class 'sklearn.linear_model.logistic.LogisticRegression'>\n",
      "crossvalscore mean = 0.7684956843480272\n",
      "\n",
      "vectorizer - <class 'sklearn.feature_extraction.text.CountVectorizer'>\n",
      "classifier - <class 'sklearn.svm.classes.LinearSVC'>\n",
      "crossvalscore mean = 0.754000653129082\n",
      "\n",
      "vectorizer - <class 'sklearn.feature_extraction.text.CountVectorizer'>\n",
      "classifier - <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'>\n",
      "crossvalscore mean = 0.7404818436365227\n",
      "\n",
      "vectorizer - <class 'sklearn.feature_extraction.text.TfidfVectorizer'>\n",
      "classifier - <class 'sklearn.linear_model.logistic.LogisticRegression'>\n",
      "crossvalscore mean = 0.7665031843949025\n",
      "\n",
      "vectorizer - <class 'sklearn.feature_extraction.text.TfidfVectorizer'>\n",
      "classifier - <class 'sklearn.svm.classes.LinearSVC'>\n",
      "crossvalscore mean = 0.7684856717854487\n",
      "\n",
      "vectorizer - <class 'sklearn.feature_extraction.text.TfidfVectorizer'>\n",
      "classifier - <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'>\n",
      "crossvalscore mean = 0.7559868936680854\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for vec in [CountVectorizer, TfidfVectorizer]:\n",
    "    for clf in [LogisticRegression, LinearSVC, SGDClassifier]:\n",
    "        print(\"vectorizer -\", vec)\n",
    "        print(\"classifier -\", clf)        \n",
    "        print('crossvalscore mean =', cross_val_score(text_classifier(vec(), clf()), texts, labels, cv=5).mean())\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Выбираем CountVectorizer и LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Загружаем стоп слова из nltk\n",
    "stop_words = set(stopwords.words('english')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7540043687773048\n"
     ]
    }
   ],
   "source": [
    "text_classifier = Pipeline([(\"vectorizer\", CountVectorizer(stop_words=stop_words)),\n",
    "                            (\"classifier\", LogisticRegression())])\n",
    "\n",
    "print(cross_val_score(text_classifier, texts, labels, cv=5).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7469993812461327\n"
     ]
    }
   ],
   "source": [
    "text_classifier = Pipeline([(\"vectorizer\", CountVectorizer(stop_words='english')),\n",
    "                            (\"classifier\", LogisticRegression())])\n",
    "\n",
    "print(cross_val_score(text_classifier, texts, labels, cv=5).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Результат только ухудшился, не будем использовать стоп слова"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Попробуем в CountVectorizer добавить к словам биграммы и измерить качество модели."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i = 1\n",
      "j = 1\n",
      "0.7684956843480272\n",
      "\n",
      "i = 1\n",
      "j = 2\n",
      "0.7705044437777736\n",
      "\n",
      "i = 1\n",
      "j = 3\n",
      "0.7665106656916605\n",
      "\n",
      "i = 2\n",
      "j = 2\n",
      "0.7159955530972069\n",
      "\n",
      "i = 2\n",
      "j = 3\n",
      "0.6969954905968162\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(1,3):\n",
    "    for j in range(i,4):        \n",
    "        print('i =', i)\n",
    "        print('j =', j)\n",
    "        text_classifier = Pipeline([(\"vectorizer\", CountVectorizer(ngram_range=(i, j))),\n",
    "                            (\"classifier\", LogisticRegression())])\n",
    "\n",
    "        print(cross_val_score(text_classifier, texts, labels, cv=5).mean())\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Остановимся на ngram_range=(1, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Загрузим в модель тестовые данные и составим таблицу с ответами."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Id                                               text\n",
      "0   0  so , why the small digital elph , rather than ...\n",
      "1   1  3/4 way through the first disk we played on it...\n",
      "2   2  better for the zen micro is outlook compatibil...\n",
      "3   3    6 . play gameboy color games on it with goboy .\n",
      "4   4  likewise , i 've heard norton 2004 professiona...\n",
      "(500, 2)\n",
      "\n",
      "[\"so , why the small digital elph , rather than one of the other cameras with better resolution or picture quality ? size [ + 2 ] # # because , unless it 's small , i won 't cary it around .\", '3/4 way through the first disk we played on it ( naturally on 31 days after purchase ) the dvd player froze . ', 'better for the zen micro is outlook compatibility .', '6 . play gameboy color games on it with goboy .', \"likewise , i 've heard norton 2004 professional version is fine too .\"]\n"
     ]
    }
   ],
   "source": [
    "test = pd.read_csv('products_sentiment_test.tsv', sep = '\\t')\n",
    "print(test.head(5))\n",
    "print(test.shape)\n",
    "print()\n",
    "test_texts = list(test['text'])\n",
    "print(test_texts[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 1 1 0 0 1 0 0 1 1 0 1 0 1 1 1 0 1 1 1 1 0 1 1 0 1 0 1 1 1 0 1 1 1 1 1\n",
      " 1 1 0 0 1 1 1 1 0 1 1 1 0 1 0 1 1 0 1 1 0 1 1 1 1 1 0 0 1 0 0 1 1 1 0 1 0\n",
      " 1 0 0 0 1 1 1 0 0 1 1 1 1 1 1 0 1 1 1 0 1 1 1 1 0 1 1 1 0 1 1 0 1 1 1 1 1\n",
      " 1 0 1 1 1 1 1 1 0 0 1 0 1 1 0 1 1 1 1 1 0 1 1 1 1 0 1 0 1 1 0 1 0 1 1 0 0\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 0 1 0 1 1 1 1 1 0 1 1 1 0 1 1 0 1 0 1 0 1 1\n",
      " 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 1 1 1 0 1 0 1 1 1 0 1 1\n",
      " 1 1 1 1 0 0 1 0 1 1 1 1 1 0 1 1 1 0 1 1 1 1 1 0 1 0 1 1 1 1 1 1 1 1 1 1 0\n",
      " 1 1 0 0 1 0 0 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 1 0 1 1 1 1 0 1 1 0\n",
      " 1 1 1 1 0 1 1 1 0 1 0 1 0 1 1 1 1 1 1 0 1 1 0 1 1 0 1 0 1 1 1 1 1 1 1 1 0\n",
      " 1 1 1 1 1 1 1 1 0 0 0 1 1 1 1 1 0 1 0 1 0 1 1 1 1 0 1 1 0 1 1 0 1 1 1 1 0\n",
      " 1 1 1 1 1 0 0 0 0 1 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1\n",
      " 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 1 1 1 1 1 1 0 1 1 1 0 1 1 1 1\n",
      " 0 1 0 1 1 1 0 0 0 1 0 0 1 1 1 0 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 0 0 1\n",
      " 1 1 0 1 1 1 1 1 1 1 0 1 1 1 0 1 0 0 1]\n"
     ]
    }
   ],
   "source": [
    "clf_pipeline = Pipeline([(\"vectorizer\", CountVectorizer(ngram_range=(1, 2))),\n",
    "                         (\"classifier\", LogisticRegression())])\n",
    "clf_pipeline.fit(texts, labels)\n",
    "predict = clf_pipeline.predict(test_texts)\n",
    "print(predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_answer_1(optimal_d):\n",
    "    with open(\"clf_sentiment.csv\", \"w\") as fout:\n",
    "        fout.write('Id,y')\n",
    "        fout.write('\\n')\n",
    "        for i in range(len(test)):\n",
    "            fout.write(str(test['Id'][i]))\n",
    "            fout.write(',')\n",
    "            fout.write(str(predict[i]))\n",
    "            fout.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_answer_1(predict_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
